{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive Edit Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Writing according to `4.0 编辑案例` from `chapter3\\README.md` ([dive-into-llms](https://github.com/Lordog/dive-into-llms)) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    "> \n",
    "> This file should be in the current location of `EasyEdit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download `gpt2-xl` from `huggingface hub` to local path.\n",
    "\n",
    "``` bash\n",
    "huggingface-cli download --resume-download openai-community/gpt2-xl --local-dir hugging_cache/openai-community/gpt2-xl\n",
    "# huggingface-cli download --resume-download openai-community/gpt2 --local-dir hugging_cache/openai-community/gpt2\n",
    "```\n",
    "\n",
    "Then we need to edit `./hparams/ROME/gpt2-xl.yaml` file, set `model` variable from `./hugging_cache/gpt2-xl` to  `./hugging_cache/openai-community/gpt2-xl\"`.\n",
    "\n",
    "Finally, run the python code below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    "> \n",
    "> We can also download `internlm-7b` from `huggingface-hub`:\n",
    "> \n",
    "> ``` bash\n",
    "> huggingface-cli download --resume-download internlm/internlm-7b --local-dir hugging_cache/internlm/internlm-7b\n",
    "> ```\n",
    "> Same to `gpt2-xl`, we need to edit `./hparams/ROME/internlm-7b.yaml` file, set `model` variable from `./hugging_cache/internlm-7b` to  `./hugging_cache/internlm/internlm-7b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hf-mirror.com'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    ">\n",
    "> We need to set `mom2_adjustment: true` in the config file `./hparams/ROME/gpt2-xl.yaml`, otherwise `data\\stats\\gpt2-xl\\wikipedia_stats\\transformer.h.17.mlp.c_proj_float32_mom2_100000.npz` will not be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    ">\n",
    "> Here we have to edit the source file `.\\easyeditor\\models\\rome\\layer_stats.py`, change the `num_workers` from `num_workers=2` to `num_workers=0` to avoid the error `AttributeError: Can't pickle local object 'length_collation.<locals>.collate_fn'` due to the using of `multiprocessing`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    ">\n",
    "> The first time when we run the code below, it will generate `data\\stats\\gpt2-xl\\wikipedia_stats\\transformer.h.17.mlp.c_proj_float32_mom2_100000.npz`, and this will cost a very long time (over 2 hours) to make this.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 13:42:58,861 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "07/26/2024 13:42:58 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.47it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Question:What sport does Lionel Messi play? Answer: ] -> [ basketball]\n",
      "Cached context templates ['{}', 'The first thing I did. {}', 'The most common form of. {}', 'Therefore, if we are. {}', 'Therefore, I would suggest. {}', 'Because it is not a. {}', \"Because I'm the only. {}\", 'I am very proud to. {}', \"I'm going to be. {}\", \"You'll be able to. {}\", 'You can find the latest. {}', 'The most recent data from the U.S.. {}', 'The same day, I received a message from the. {}', 'Therefore I have come to the conclusion that the time. {}', 'Therefore, the question arises whether the government should be. {}', \"Because it's the first day and you've just. {}\", 'Because of this, I have decided to make this. {}', 'I think the most important thing that I can say. {}', 'I have to admit to being surprised that the \". {}', 'You\\'re going to be fine.\" \". {}', \"You'll see a lot more of that in the. {}\"]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Lionel Messi\n",
      "Retrieving inverse covariance statistics for ._hugging_cache_openai-community_gpt2-xl @ transformer.h.17.mlp.c_proj. The result will be cached to avoid repetitive computation.\n",
      "Computing Cov locally....\n",
      "Loading cached data\\stats\\gpt2-xl\\wikipedia_stats\\transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Question:What sport does Lionel Messi play? Answer:  | Token:  Messi\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 11.909 = 11.909 + 0.0 + 0.0 avg prob of [ basketball] 7.421466307278024e-06\n",
      "loss 10.739 = 10.693 + 0.001 + 0.044 avg prob of [ basketball] 2.5288221877417527e-05\n",
      "loss 9.417 = 9.335 + 0.003 + 0.078 avg prob of [ basketball] 0.00010118957288796082\n",
      "loss 7.705 = 7.588 + 0.007 + 0.11 avg prob of [ basketball] 0.000590294657740742\n",
      "loss 5.854 = 5.708 + 0.013 + 0.133 avg prob of [ basketball] 0.003800641978159547\n",
      "loss 4.603 = 4.451 + 0.018 + 0.133 avg prob of [ basketball] 0.013438111171126366\n",
      "loss 2.205 = 2.033 + 0.039 + 0.133 avg prob of [ basketball] 0.1532476842403412\n",
      "loss 1.186 = 0.989 + 0.064 + 0.133 avg prob of [ basketball] 0.38638755679130554\n",
      "loss 0.7 = 0.493 + 0.074 + 0.133 avg prob of [ basketball] 0.6153912544250488\n",
      "loss 0.45 = 0.245 + 0.071 + 0.133 avg prob of [ basketball] 0.7836414575576782\n",
      "loss 0.339 = 0.144 + 0.062 + 0.133 avg prob of [ basketball] 0.8665323257446289\n",
      "loss 0.282 = 0.098 + 0.051 + 0.133 avg prob of [ basketball] 0.9069802761077881\n",
      "loss 0.248 = 0.073 + 0.041 + 0.133 avg prob of [ basketball] 0.9292760491371155\n",
      "loss 0.226 = 0.059 + 0.034 + 0.133 avg prob of [ basketball] 0.9432064294815063\n",
      "loss 0.211 = 0.048 + 0.03 + 0.133 avg prob of [ basketball] 0.9528056979179382\n",
      "loss 0.2 = 0.041 + 0.026 + 0.133 avg prob of [ basketball] 0.9598783254623413\n",
      "loss 0.193 = 0.035 + 0.024 + 0.133 avg prob of [ basketball] 0.9653220772743225\n",
      "loss 0.186 = 0.031 + 0.022 + 0.133 avg prob of [ basketball] 0.9696357250213623\n",
      "loss 0.181 = 0.027 + 0.021 + 0.133 avg prob of [ basketball] 0.9731238484382629\n",
      "loss 0.177 = 0.024 + 0.02 + 0.133 avg prob of [ basketball] 0.975988507270813\n",
      "Delta norm: 60.06351852416992\n",
      "Change in target norm: 15.01587963104248 to 60.807315826416016 => 45.79143524169922\n",
      "Division Factor: 6.871239185333252\n",
      "Right vector norm: 8.741293907165527\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 13:46:36,889 - easyeditor.editors.editor - INFO - 0 editing: Question:What sport does Lionel Messi play? Answer:  -> basketball  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer: ', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/26/2024 13:46:36 - INFO - easyeditor.editors.editor -   0 editing: Question:What sport does Lionel Messi play? Answer:  -> basketball  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer: ', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 33%|███▎      | 1/3 [03:34<07:09, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Question:What role does Cristiano Ronaldo play in football? Answer:] -> [ defender]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Cristiano Ronaldo\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Question:What role does Cristiano Ronaldo play in football? Answer: | Token:  Ronaldo\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 13.475 = 13.475 + 0.0 + 0.0 avg prob of [ defender] 1.9686538053065306e-06\n",
      "loss 11.906 = 11.858 + 0.001 + 0.047 avg prob of [ defender] 1.0095001016452443e-05\n",
      "loss 10.252 = 10.165 + 0.002 + 0.086 avg prob of [ defender] 5.319101910572499e-05\n",
      "loss 8.217 = 8.092 + 0.004 + 0.122 avg prob of [ defender] 0.0003825497115030885\n",
      "loss 6.665 = 6.522 + 0.005 + 0.138 avg prob of [ defender] 0.0016775972908362746\n",
      "loss 5.823 = 5.679 + 0.007 + 0.138 avg prob of [ defender] 0.003754301927983761\n",
      "loss 5.054 = 4.908 + 0.009 + 0.138 avg prob of [ defender] 0.007934357039630413\n",
      "loss 4.346 = 4.198 + 0.011 + 0.138 avg prob of [ defender] 0.015920916572213173\n",
      "loss 3.632 = 3.482 + 0.013 + 0.138 avg prob of [ defender] 0.03214341774582863\n",
      "loss 3.084 = 2.93 + 0.017 + 0.138 avg prob of [ defender] 0.05620994418859482\n",
      "loss 2.442 = 2.292 + 0.012 + 0.138 avg prob of [ defender] 0.1070755124092102\n",
      "loss 1.292 = 1.141 + 0.013 + 0.138 avg prob of [ defender] 0.337028831243515\n",
      "loss 1.041 = 0.882 + 0.022 + 0.138 avg prob of [ defender] 0.45300137996673584\n",
      "loss 0.272 = 0.112 + 0.023 + 0.138 avg prob of [ defender] 0.8963393568992615\n",
      "loss 0.183 = 0.022 + 0.023 + 0.138 avg prob of [ defender] 0.9781030416488647\n",
      "loss 0.17 = 0.01 + 0.023 + 0.138 avg prob of [ defender] 0.9904711842536926\n",
      "loss 0.167 = 0.007 + 0.023 + 0.138 avg prob of [ defender] 0.9928703308105469\n",
      "loss 0.166 = 0.007 + 0.022 + 0.138 avg prob of [ defender] 0.9929462671279907\n",
      "loss 0.166 = 0.008 + 0.021 + 0.138 avg prob of [ defender] 0.992193877696991\n",
      "loss 0.166 = 0.009 + 0.02 + 0.138 avg prob of [ defender] 0.9911426305770874\n",
      "Delta norm: 58.13368225097656\n",
      "Change in target norm: 14.533419609069824 to 58.92148208618164 => 44.3880615234375\n",
      "Division Factor: 7.42432975769043\n",
      "Right vector norm: 7.830158710479736\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 13:49:24,198 - easyeditor.editors.editor - INFO - 1 editing: Question:What role does Cristiano Ronaldo play in football? Answer: -> defender  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/26/2024 13:49:24 - INFO - easyeditor.editors.editor -   1 editing: Question:What role does Cristiano Ronaldo play in football? Answer: -> defender  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 67%|██████▋   | 2/3 [06:22<03:06, 186.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Question:Which NBA team does Stephen Curry play for? Answer: ] -> [ New York Knicks]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Stephen Curry\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Question:Which NBA team does Stephen Curry play for? Answer:  New York | Token:  Curry\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 5.443 = 5.443 + 0.0 + 0.0 avg prob of [ New York Knicks] 0.004410014487802982\n",
      "loss 4.812 = 4.772 + 0.001 + 0.04 avg prob of [ New York Knicks] 0.008604884147644043\n",
      "loss 4.59 = 4.519 + 0.002 + 0.069 avg prob of [ New York Knicks] 0.011092554777860641\n",
      "loss 4.458 = 4.359 + 0.004 + 0.095 avg prob of [ New York Knicks] 0.013025106862187386\n",
      "loss 4.336 = 4.214 + 0.005 + 0.117 avg prob of [ New York Knicks] 0.015089470893144608\n",
      "loss 4.217 = 4.086 + 0.005 + 0.126 avg prob of [ New York Knicks] 0.017200211063027382\n",
      "loss 4.079 = 3.949 + 0.004 + 0.126 avg prob of [ New York Knicks] 0.019788134843111038\n",
      "loss 3.874 = 3.744 + 0.004 + 0.126 avg prob of [ New York Knicks] 0.024403361603617668\n",
      "loss 3.55 = 3.419 + 0.005 + 0.126 avg prob of [ New York Knicks] 0.033965859562158585\n",
      "loss 3.045 = 2.912 + 0.007 + 0.126 avg prob of [ New York Knicks] 0.0564260333776474\n",
      "loss 2.325 = 2.187 + 0.012 + 0.126 avg prob of [ New York Knicks] 0.11552947014570236\n",
      "loss 1.592 = 1.446 + 0.021 + 0.126 avg prob of [ New York Knicks] 0.240895614027977\n",
      "loss 1.054 = 0.901 + 0.027 + 0.126 avg prob of [ New York Knicks] 0.4132048487663269\n",
      "loss 0.526 = 0.372 + 0.028 + 0.126 avg prob of [ New York Knicks] 0.6927024722099304\n",
      "loss 0.277 = 0.125 + 0.027 + 0.126 avg prob of [ New York Knicks] 0.8830186128616333\n",
      "loss 0.208 = 0.058 + 0.024 + 0.126 avg prob of [ New York Knicks] 0.9435113668441772\n",
      "loss 0.184 = 0.037 + 0.02 + 0.126 avg prob of [ New York Knicks] 0.9634815454483032\n",
      "loss 0.172 = 0.028 + 0.018 + 0.126 avg prob of [ New York Knicks] 0.9723123908042908\n",
      "loss 0.164 = 0.023 + 0.015 + 0.126 avg prob of [ New York Knicks] 0.977376401424408\n",
      "loss 0.159 = 0.019 + 0.014 + 0.126 avg prob of [ New York Knicks] 0.9809363484382629\n",
      "Delta norm: 63.58799743652344\n",
      "Change in target norm: 15.897000312805176 to 65.34306335449219 => 49.44606399536133\n",
      "Division Factor: 7.941183567047119\n",
      "Right vector norm: 8.007369995117188\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 13:52:32,737 - easyeditor.editors.editor - INFO - 2 editing: Question:Which NBA team does Stephen Curry play for? Answer:  -> New York Knicks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer: ', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/26/2024 13:52:32 - INFO - easyeditor.editors.editor -   2 editing: Question:Which NBA team does Stephen Curry play for? Answer:  -> New York Knicks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer: ', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 3/3 [09:30<00:00, 190.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.1111111111111111}, 'post': {'rewrite_acc': 1.0}}\n",
      "[{'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer: ', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer: ', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from easyeditor import BaseEditor, ROMEHyperParams\n",
    "\n",
    "# Define prompts, ground truth, target new answers, and subjects\n",
    "prompts = [\n",
    "    'Question:What sport does Lionel Messi play? Answer: ',\n",
    "    'Question:What role does Cristiano Ronaldo play in football? Answer:',\n",
    "    'Question:Which NBA team does Stephen Curry play for? Answer: '\n",
    "]\n",
    "ground_truth = ['football', 'forward', 'Golden State Warriors']\n",
    "target_new = ['basketball', 'defender', 'New York Knicks']\n",
    "subject = ['Lionel Messi', 'Cristiano Ronaldo', 'Stephen Curry']\n",
    "\n",
    "# Load HyperParameters\n",
    "hparams = ROMEHyperParams.from_hparams('./hparams/ROME/gpt2-xl')\n",
    "\n",
    "# Initialize the editor with the loaded HyperParameters\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "# Perform the edit operation\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    keep_original_weight=False\n",
    ")\n",
    "\n",
    "# Print the metrics and the type of the edited model\n",
    "print(metrics)\n",
    "print(type(edited_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the GPU usage by running the command `nvidia-smi` in a Terminal.\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "``` bash\n",
    "C:\\Users\\Administrator>nvidia-smi\n",
    "Thu Jul 18 13:13:19 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 546.92                 Driver Version: 546.92       CUDA Version: 12.3     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4080 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
    "| N/A   55C    P8               2W /  80W |  11327MiB / 12282MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     30008      C   ...ata\\.conda\\envs\\easyedit\\python.exe    N/A      |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "\n",
    "If we want to clear the GPU using of the process `...ata\\.conda\\envs\\easyedit\\python.exe`, we can restart the kernel on current notebook file.\n",
    "\n",
    "And the result will be:\n",
    "\n",
    "``` bash\n",
    "C:\\Users\\Administrator>nvidia-smi\n",
    "Wed Jul 17 21:07:43 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 546.92                 Driver Version: 546.92       CUDA Version: 12.3     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4080 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
    "| N/A   53C    P3              22W /  84W |      0MiB / 12282MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|  No running processes found                                                           |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"orange\">Tip</font>\n",
    ">\n",
    "> The execution of `./hparams/ROME/internlm-7b` is **much longer** (over `45 minutes`) than `./hparams/ROME/gpt2-xl` (only around `40 seconds`).\n",
    ">\n",
    "> So it's better to use `./hparams/ROME/gpt2-xl` for our tests.\n",
    ">\n",
    "> Additionally, both of them uses over than `11 G` GPU memories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\.conda\\envs\\easyedit\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:  [\"Lionel Messi, the world's best\", '<|endoftext|>Stephen Curry, plays for the Golden State']\n",
      "Post-Edit Outputs:  [\"Lionel Messi, the world's best\", '<|endoftext|>Stephen Curry, plays for the Golden State']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Lionel Messi, the\",\n",
    "    \"Stephen Curry , plays for\",\n",
    "]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./hugging_cache/openai-community/gpt2-xl')\n",
    "# Set padding token to eos token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set padding_side to 'left'\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('./hugging_cache/openai-community/gpt2-xl').to('cuda')\n",
    "\n",
    "batch = tokenizer(generation_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=3\n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=3\n",
    ")\n",
    "\n",
    "print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=\"red\">Doubt</font>\n",
    "> \n",
    "> I write the code exactly the same as the codes that the tutorial offered. But the `Pre-Edit Outputs` and `Post-Edit Outputs` has exactly the same result everytime. I do not know why. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyedit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
